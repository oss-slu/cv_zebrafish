# CV_Zebrafish

## Overview
Computer-vision toolkit for analyzing zebrafish movement data exported from DeepLabCut. The project ships a PyQt UI, calculation pipeline, validation utilities, Plotly-based visualizations, and historical parity checks against the legacy “Bruce” workflow.

## Repository Layout
```text
.
├── app.py / webengineDemo.py         # PyQt entry point and Plotly demo
├── assets/
│   ├── images/                       # UI illustrations and icons
│   └── sample_data/                  # Placeholder for bundled DLC runs
├── configs/
│   ├── defaults/                     # Reserved for auto-generated configs
│   └── samples/
│       ├── csv/                      # Curated DeepLabCut exports
│       └── jsons/                    # BaseConfig + autogenerated variants
├── docs/
│   ├── architecture/calculations/    # Implementation notes + comparisons
│   ├── howtos/{ui,validation}/       # Step-by-step guides
│   └── product/{meeting_minutes,...} # Product docs and presentations
├── legacy/
│   ├── codes/                        # Bruce pipeline + configs/results
│   ├── dlc/, input_data/             # Historical DLC artifacts
│   └── reports/                      # Output CSVs, QA assets
├── requirements/validation/          # Narrow dependency set for validators
├── scripts/
│   ├── data/generate_graphs.py       # CLI to persist Plotly figures
│   ├── dev/                          # Reserved for local tooling
│   └── ops/                          # Reserved for automation hooks
├── src/cvzebrafish/
│   ├── core/
│   │   ├── calculations/{Driver,Metrics,...}
│   │   ├── config/configSetup.py     # Config discovery helpers
│   │   ├── parsing/Parser.py         # DeepLabCut CSV parser
│   │   └── validation/{csv,json}_verifier.py
│   ├── data/sources/DB1.py           # SQLite ingestion and normalization
│   ├── orchestrators/                # Reserved for workflow drivers
│   ├── platform/paths.py             # Centralized repo path helpers
│   ├── ui/{components,scenes}/       # PyQt widgets & multi-scene flow
│   └── viz/{adapters,figures}/       # Plotly export + pipeline adapters
├── tests/
│   ├── unit/core/{calculations,validation}/
│   ├── integration/, e2e/ (stubs)
│   └── conftest.py + shared fixtures
├── AGENTS.md, contributing.md, LICENSE, environment.yml, requirements.txt
└── package*.json                     # Front-end prototype scaffolding
```

## Core Functionality Highlights
- **Parsing & Config Loading** – `src/cvzebrafish/core/parsing/Parser.py` maps DLC CSV columns into structured point dictionaries, while `core/config/configSetup.py` cascades through local/sample config locations and backfills missing files from `BaseConfig.json`.
- **Calculation Pipeline** – `core/calculations/Driver.py` orchestrates kinematic metrics (angles, yaw, bout detection) via helpers in `Metrics.py`. CLI utilities `core/calculations/run_calculation_to_csv.py` and `core/calculations/compare_calculations.py` export results or diff them against the legacy Bruce pipeline.
- **Validation Utilities** – `core/validation/json_verifier.py`, `csv_verifier.py`, and `generate_json.py` enforce schema integrity, check DLC CSV structure/ranges, and scaffold configs. Minimal dependencies live in `requirements/validation/requirements.txt`.
- **Visualization & Export** – `viz/adapters/output_adapter.py` and `viz/figures/outputDisplay.py` turn calculation outputs into Plotly charts plus CSV/HTML artifacts; `scripts/data/generate_graphs.py` wires the pipeline together for headless rendering.
- **PyQt UI Flow** – `app.py` boots the multi-scene UI defined under `src/cvzebrafish/ui/scenes` (Landing, CSV/JSON inputs, Config generators, Calculation/Graph viewer, Validators) and reuses shared widgets from `ui/components/`.
- **Data Ingestion & Persistence** – `src/cvzebrafish/data/sources/DB1.py` normalizes DLC outputs into SQLite (`CV_Zebrafish.db`), tracks file hashes, and records ingestion runs for reproducibility.
- **Legacy Parity & Docs** – `legacy/` mirrors the prior code path so regression comparisons and documentation in `docs/architecture/.../comparison_legacy_vs_new.md` stay grounded.
- **Automated Tests** – `tests/unit/core/calculations` exercises parser + metrics against golden CSV fixtures, while `tests/unit/core/validation` covers schema/CSV tooling. `pytest` is configured via `conftest.py`.

## Python Environment
You can use Conda or a virtual environment created with `python -m venv`.

### Option A: pip / venv
```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS / Linux
source .venv/bin/activate
pip install -r requirements.txt
```

### Option B: Conda
```bash
conda env create -f environment.yml
conda activate dlc
```
Deactivate the environment with `deactivate` (venv) or `conda deactivate` when you finish.

## Running the App
From the repository root:
```bash
python app.py
```
The UI walks through CSV/JSON selection, validation, configuration tweaks, calculation runs, and graph review.

### Useful CLI Utilities
- `python src/cvzebrafish/core/calculations/run_calculation_to_csv.py --csv <dlc.csv> --config <config.json> [--output results.csv]`
- `python src/cvzebrafish/core/calculations/compare_calculations.py --csv <dlc.csv> --config <config.json> [--legacy-root legacy/codes]`
- `python scripts/data/generate_graphs.py --csv <dlc.csv> --config <config.json> --output results/`
- `pytest` to execute the unit suites
- `python -m cvzebrafish.core.validation.json_verifier <config.json>` (or run without arguments to be prompted for a path)
- `python -m cvzebrafish.core.validation.csv_verifier` to be prompted for a DLC CSV and run structural checks

## License
This project is licensed under the [MIT License](./LICENSE) — see the file for details.
