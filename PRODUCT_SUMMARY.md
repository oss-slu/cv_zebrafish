# CV Zebrafish: Product Summary

## Boardroom Presentation Reference Document

---

## Executive Overview

### The Problem

Behavioral neuroscience researchers studying zebrafish movement face significant technical barriers when analyzing motion tracking data. While tools like DeepLabCut can capture movement coordinates, converting this raw data into meaningful insights requires:

- Manual data processing and validation
- Programming expertise in Python and plotting libraries
- Time-consuming, error-prone graph generation
- Technical knowledge that many researchers lack

These inefficiencies slow research progress, create bottlenecks in data analysis pipelines, and limit accessibility for researchers without computational backgrounds.

### The Solution

**CV Zebrafish** is a user-friendly desktop application that transforms DeepLabCut tracking data into publication-ready movement visualizations with minimal technical expertise required. The software handles the entire pipeline from data validation to graph generation, abstracting away complexity while maintaining scientific rigor.

### Target Users

- **Primary:** Dr. Mohini Sengupta's Bio Lab at Saint Louis University
- **Secondary:** Broader zebrafish behavioral neuroscience research community
- **Future:** Labs studying other aquatic organisms and behavioral tracking applications

### Value Proposition

CV Zebrafish delivers measurable time savings and accessibility improvements:

- **Reduces analysis time** from hours of manual scripting to minutes of point-and-click interaction
- **Democratizes access** by removing programming barriers for non-technical researchers
- **Ensures consistency** through standardized, validated data processing
- **Accelerates research** by streamlining the path from raw data to scientific insights

---

## Product Description

### What It Does

CV Zebrafish is a desktop application built with Python and PyQt5 that provides an intuitive graphical interface for zebrafish movement analysis. Researchers can:

1. **Load DeepLabCut CSV files** containing tracked body part coordinates
2. **Validate data quality** automatically against expected format requirements
3. **Generate configuration files** that define which body parts to analyze
4. **Calculate kinematic metrics** including fin angles, head yaw, tail motion, and spine curvature
5. **Visualize results** as interactive, publication-ready graphs

### Key Features

#### 1. Data Validation System

- Automatically verifies CSV structure matches DeepLabCut output format
- Checks for required body parts (head points, fins, tail, spine)
- Validates data types and likelihood scores
- Provides clear error messages for malformed inputs

#### 2. Configuration Generator

- Extracts available body parts from CSV files
- Builds JSON configuration files through guided UI
- Saves configurations for reuse across similar experiments
- Supports customization of analysis parameters

#### 3. Kinematic Analysis Engine

- **Fin Angles:** Left and right pectoral fin angles relative to body axis
- **Head Yaw:** Orientation and heading direction over time
- **Tail Motion:** Side (left/right), distance from centerline, and curvature
- **Spine Analysis:** Multi-segment angular deformation patterns
- **Swim Bout Detection:** Automatic identification of movement periods
- **Peak Detection:** Identification of maximum fin extension points

#### 4. Visualization System

- Plotly-based interactive graphs rendered as high-resolution static images
- Multiple plot types: angle-over-time, movement tracks, heatmaps, spine deformation
- Sidebar navigation for switching between graph types
- Scalable display adapting to window size

### User Workflow

```
CSV Upload â†’ Data Validation â†’ Config Generation/Selection â†’
Kinematic Calculations â†’ Graph Visualization â†’ Export
```

The entire process can be completed in under 5 minutes for a typical dataset, compared to 1-2 hours of manual Python scripting.

---

## Technical Implementation

### Architecture Overview

CV Zebrafish follows a modular, layered architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PyQt5 GUI (Frontend)            â”‚
â”‚  CSV/JSON Input â€¢ Config Gen â€¢ Results  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Data Validation Layer              â”‚
â”‚   CSV Verifier â€¢ JSON Verifier          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Calculation Engine                 â”‚
â”‚   Parser â€¢ Metrics â€¢ Driver             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Visualization Layer                â”‚
â”‚   Plotly Figure Generation â€¢ Display    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Each layer communicates through well-defined interfaces, making the system maintainable and testable.

### Technology Stack

#### Core Technologies

- **Python 3.10:** Industry-standard language for scientific computing
- **PyQt5:** Professional desktop GUI framework with native look-and-feel
- **NumPy/Pandas:** Efficient numerical computation and data manipulation
- **Plotly:** Publication-quality interactive visualizations
- **Conda:** Reproducible environment management

#### Key Libraries

- **DeepLabCut:** Integration with pose estimation pipeline
- **OpenCV:** Image processing capabilities
- **scikit-learn/scipy:** Statistical analysis tools
- **matplotlib:** Additional plotting support

### Major Technical Decisions

#### 1. Why Python?

- **Ecosystem compatibility:** DeepLabCut and the original prototype were Python-based
- **Scientific computing:** NumPy/Pandas/SciPy provide battle-tested numerical libraries
- **Rapid development:** Python enables quick iteration for research software
- **Lab familiarity:** The Sengupta Lab has existing Python expertise

#### 2. Why PyQt5 over Web Technologies?

- **Desktop-first:** No internet connection required; works on isolated research computers
- **Performance:** Native rendering for large datasets and complex visualizations
- **Professional appearance:** Native OS integration creates familiar user experience
- **File system access:** Direct CSV/JSON handling without browser security restrictions

#### 3. Modular Architecture

- **Testability:** Each component can be unit tested independently
- **Maintainability:** Clear separation of concerns simplifies debugging and updates
- **Extensibility:** New metrics or visualizations can be added without touching existing code
- **Collaboration:** Team members can work on different modules in parallel

#### 4. Configuration-Driven Design

- **Flexibility:** Users can customize analysis without code changes
- **Reproducibility:** JSON configs serve as analysis documentation
- **Batch processing:** Same config can be reused across multiple datasets
- **Versioning:** Config files can be tracked in version control for experiment provenance

---

## Work Accomplished This Iteration

### Starting Point (September 2025)

- Basic prototype Python scripts from Sengupta Lab
- Manual command-line execution
- Limited documentation
- No user interface

### Current State (October 2025)

#### âœ… Fully Implemented Features

**1. User Interface Framework (frontend/widgets/)**

- Main window with toolbar navigation (`MainWindow.py` - 79 lines)
- Five integrated scenes: CSV Input, JSON Input, Config Generator, Calculation, Graph Viewer
- Professional layout with 1200Ã—800 starting window size
- Keyboard shortcuts (Ctrl+W to close)
- Signal-based communication between components

**2. Data Validation System (data_schema_validation/)**

- CSV format verifier (`csv_verifier.py`)
- JSON configuration verifier (`json_verifier.py`)
- Comprehensive unit tests (`test/unit_test.py`)
- Sample inputs for testing (`sample_inputs/`)
- Detailed format documentation

**3. Configuration Management**

- Auto-generation of JSON configs from CSV files (`generate_json.py`)
- Base configuration with 97 lines of structured parameters
- Support for:
  - Body part mapping (head, fins, tail, spine)
  - Video parameters (framerate, scale factors, dish dimensions)
  - Plot settings (spine plots, head plots, angle/distance plots)
  - Swim bout detection thresholds
  - Time range specification

**4. Calculation Engine (calculations/utils/)**

- **Parser (`Parser.py`):** Converts DLC CSV format into structured dictionaries
- **Metrics (`Metrics.py` - 254 lines):**
  - `calc_fin_angle()`: Fin angles relative to body centerline
  - `calc_yaw()`: Head orientation in degrees
  - `calc_spine_angles()`: Multi-segment spine deformation
  - `calc_tail_angle()`: Tail angle from caudal peduncle
  - `calc_tail_side_and_distance()`: Tail lateral displacement with sign
  - `calc_furthest_tail_point()`: Peak tail deflection identification
  - `detect_fin_peaks()`: Local maxima/minima detection with configurable buffer
  - `get_time_ranges()`: Automatic swim bout detection based on movement thresholds
- **Driver (`Driver.py` - 92 lines):** Orchestrates the full calculation pipeline
  - Accepts parsed points and configuration
  - Computes all kinematic metrics
  - Outputs structured DataFrame with results
  - Handles video scaling (pixel-to-metric conversion)
  - Swim bout-relative head yaw computation

**5. Calculation Scene Integration**

- File selection dialogs for CSV and JSON inputs (`CalculationScene.py` - 172 lines)
- Toggle between test data and user data
- Visual status panel showing loaded files
- "Run Calculation" button that becomes active when inputs are ready
- Progress indicators and error handling
- Signal emission to trigger graph generation

**6. Graph Viewer Infrastructure**

- Plotly figure display system (`GraphViewerScene.py` - 164 lines)
- Sidebar list of available graphs
- Main display area with automatic scaling
- Kaleido-based PNG rendering for static display
- Graceful handling of missing graphs or render errors
- Responsive layout that adapts to window resizing

**7. Testing Infrastructure**

- Unit tests for Driver (`test_Driver.py`)
- Unit tests for Parser (`test_Parser.py`)
- Unit tests for Metrics (`test_Calculations.py`)
- Edge case handling (zero-length centerlines, missing data, etc.)

**8. Development Workflow**

- Conda environment configuration (`environment.yml`)
- Git workflow with feature branches
- Contributing guidelines (`contributing.md`)
- Code style standards
- Documentation requirements

#### ğŸš§ In Progress

**1. Graph Generation Functions**

- Connection between calculation results and Plotly figure objects
- Implementation of specific plot types:
  - Angle and distance plots (fin angles, tail distance, head yaw)
  - Spine deformation visualizations
  - Movement track plots
  - Heatmaps of fish position

**2. Data Export**

- Saving calculated metrics to CSV
- Exporting graphs as image files (PNG, SVG)
- Report generation

### Team Structure & Methodology

#### Team Roles

- **Tech Lead:** Madu (Madhuritha Alle) - Integration oversight, architecture decisions
- **Developers:** Nilesh, Jacob, Finn - Rotating feature development
- **Client:** Dr. Mohini Sengupta - Requirements definition, domain expertise
- **Faculty Advisor:** Dr. Daniel Shown - Technical guidance

#### Development Process

- **Methodology:** Agile-style iterative development
- **Sprint cadence:** 2-week iterations
- **Task assignment:** Rotating responsibilities to build cross-functional skills
- **Code review:** Pull requests reviewed by Tech Lead before merging
- **Version control:** Feature-branch workflow with descriptive commit messages
- **Testing:** Unit tests required for calculation modules

### Recent Development Activity (Last 10 Commits)

1. Disconnecting verification scenes from calculation scene
2. Improved window sizing (1200Ã—800 default)
3. Enhanced info display with styled panels
4. User selection buttons in calculation scene
5. JSON auto-config feature (#30)
6. Calculation function integration (#28)
7. File path adjustments for cross-platform compatibility

---

## Business Value & ROI

### Time Savings

**Before CV Zebrafish:**

- 30-60 minutes: Manually verify CSV format and data quality
- 60-90 minutes: Write Python scripts for metric calculations
- 30-45 minutes: Generate plots and adjust formatting
- **Total: 2-3 hours per dataset**

**After CV Zebrafish:**

- 2 minutes: Load CSV and auto-validate
- 1 minute: Generate or load configuration
- 2 minutes: Run calculations and view results
- **Total: 5 minutes per dataset**

**ROI Calculation:**

- Time saved per dataset: ~2.5 hours
- Researcher hourly rate (estimated): $40/hour
- Cost savings per dataset: $100
- If lab analyzes 50 datasets/year: **$5,000 annual savings**
- Over 3 years: **$15,000 in researcher time**

### Efficiency Improvements

**Reduced Error Rates:**

- Automated validation catches format errors immediately
- Standardized calculations eliminate manual transcription mistakes
- Consistent methodology across all datasets ensures reproducibility

**Accelerated Research Timelines:**

- Faster analysis enables more rapid hypothesis testing
- Quick visualization helps identify interesting patterns for follow-up
- Publication-ready graphs reduce manuscript preparation time

**Lowered Barriers to Entry:**

- Graduate students can analyze data without extensive programming training
- Lab technicians can perform routine analysis tasks
- Postdocs spend time on scientific interpretation instead of technical troubleshooting

### Competitive Advantages

**1. Domain Specialization:**

- Purpose-built for zebrafish kinematics (not a general plotting tool)
- Incorporates best practices from behavioral neuroscience
- Metrics designed specifically for fin-based swimming analysis

**2. Integration with DeepLabCut:**

- Seamless compatibility with leading pose estimation tool
- No manual data format conversion required
- Leverages existing lab workflows

**3. Open Source Strategy:**

- Community contributions can extend functionality
- No vendor lock-in or licensing fees
- Academic credibility through transparent methodology

**4. Extensibility:**

- Plugin architecture allows custom metrics
- Configuration system supports novel experimental designs
- Foundation for future multi-species support

### Scalability Potential

**Within Lab:**

- Currently supports single dataset analysis
- Future: Batch processing for high-throughput experiments
- Future: Automated analysis pipelines triggered by new data

**Across Labs:**

- Minimal setup requirements (conda environment)
- Cross-platform (Windows, macOS, Linux)
- Documentation designed for external users
- Potential for cloud-hosted version for labs without local compute resources

**Commercial Applications:**

- Pharmaceutical companies studying drug effects on zebrafish behavior
- Toxicology labs using zebrafish as model organisms
- Educational institutions teaching behavioral analysis methods

---

## Future Vision & Roadmap

### Short-Term: MVP Completion (Next Iteration)

**Critical Path:**

1. **Complete graph generation module**

   - Implement all plot types (angle/distance, spine, movement track, heatmap)
   - Connect calculation results to Plotly figure creation
   - Add customization options (colors, labels, legends)

2. **User feedback integration**

   - Beta testing with Sengupta Lab members
   - Usability study with non-technical users
   - Bug fixes and UI refinements

3. **Export functionality**

   - Save results to CSV
   - Export graphs as PNG/SVG/PDF
   - Generate analysis summary reports

4. **Documentation completion**
   - User manual with screenshots
   - Video tutorials for common workflows
   - API documentation for developers

**Expected Timeline:** 4-6 weeks

### Medium-Term: Enhanced Features (6-12 months)

**Batch Processing:**

- Analyze multiple datasets with single configuration
- Progress tracking for long-running analyses
- Comparison views across datasets

**Statistical Analysis:**

- Summary statistics (mean fin angle, peak frequencies, etc.)
- Bout-by-bout comparisons
- Automated anomaly detection

**Advanced Visualizations:**

- Animated movement reconstructions
- 3D trajectory plots
- Synchronized video playback with graph overlays

**Configuration Templates:**

- Pre-built configs for common experimental paradigms
- Shareable config library across research groups
- Version control integration for reproducible science

### Long-Term: Platform Expansion (1-3 years)

**Multi-Species Support:**

- Extend to other aquatic organisms (medaka, killifish, etc.)
- Support different body plans and swimming modes
- Configurable body part schemas

**Machine Learning Integration:**

- Behavior classification (startle response, hunting, rest)
- Predictive models for movement patterns
- Transfer learning for low-data scenarios

**Collaborative Features:**

- Cloud storage for shared datasets
- Team annotation and labeling tools
- Real-time collaboration on analysis configurations

**Broader Impact:**

- Plugin ecosystem for community-contributed metrics
- Integration with other tracking tools (SLEAP, Anipose)
- Publication of validated methodology papers
- Workshops and training programs for research community

---

## Investment Case

### Why Continue Funding This Project?

**1. Strong Foundation Delivered**
This iteration established a solid technical foundation:

- Robust data validation prevents garbage-in-garbage-out problems
- Proven calculation engine with comprehensive test coverage
- Professional UI framework ready for feature additions
- Clear architecture supports rapid future development

**2. High-Impact, Low-Hanging Fruit Ahead**
The next iteration completes the MVP with well-scoped tasks:

- Graph generation leverages existing Plotly expertise
- Export functionality is straightforward file I/O
- User testing provides concrete improvement roadmap
- No major technical risks or unknowns

**3. Validated User Need**
Dr. Sengupta's lab actively uses manual analysis methods today:

- Immediate adoption guaranteed upon MVP completion
- Real research papers will cite this tool
- Direct feedback loop ensures product-market fit

**4. Scalability Beyond Initial Client**
The zebrafish research community is substantial:

- Thousands of labs worldwide use zebrafish models
- Growing adoption of DeepLabCut creates natural user pipeline
- Open source strategy enables viral growth without marketing costs

**5. Student Development**
Continued development provides educational value:

- Real-world software engineering experience
- Scientific computing skills
- Collaboration and project management practice
- Portfolio pieces for future careers

**6. Publication and Recognition Potential**
A completed tool enables multiple outputs:

- Software publication in _Journal of Open Source Software_
- Methods paper in domain journal (_Zebrafish_, _J. Neuroscience Methods_)
- Conference presentations and demos
- University PR highlighting student-faculty-client collaboration

### Risks of Stopping Now

**1. Sunk Cost Loss**
Significant investment already made:

- ~200+ hours of student development time
- Faculty mentorship and oversight
- Client consultation and requirements gathering
- Infrastructure setup (git repo, environments, testing)

Stopping leaves this investment unrealized with no tangible outputs.

**2. Incomplete Product Has No Users**
Current state is non-functional for end users:

- Can calculate metrics but not visualize them
- No way to export or share results
- Missing critical features for research workflows

An incomplete tool provides zero value and zero return.

**3. Lost Opportunity Cost**
Without CV Zebrafish, labs continue inefficient manual methods:

- Researcher time remains bottlenecked
- Potential research questions go unasked due to analysis burden
- Students graduate before reaching analysis competency

**4. Team Momentum and Knowledge Loss**
Team has built up critical context:

- Understanding of zebrafish kinematics
- Familiarity with codebase architecture
- Established client relationship
- Working development processes

Restarting later requires re-onboarding and context rebuild.

### Expected Return on Continued Investment

**Next Iteration Investment:**

- Student time: ~150 hours (3 students Ã— 50 hours)
- Faculty oversight: ~20 hours
- Client consultation: ~10 hours
- **Total effort: ~180 hours**

**Tangible Returns:**

1. **Functional MVP** delivering immediate value to Sengupta Lab
2. **Time savings** of $5,000/year starting immediately
3. **Publication opportunities** for students, faculty, and client
4. **Open source contribution** enhancing SLU's research reputation
5. **Portfolio projects** helping students secure internships/jobs
6. **Foundation for grants** (NSF POSE, NIH research tools, etc.)

**Intangible Returns:**

1. Strengthened university-research collaboration model
2. Demonstration of OSS course real-world impact
3. Student confidence and software engineering skills
4. Client satisfaction enabling future collaborations

**Break-Even Analysis:**

- If tool saves 100 hours of researcher time over 2 years
- At $40/hour researcher cost: $4,000 saved
- Development cost (180 hours Ã— $25/hour student rate): $4,500
- **Break-even in ~2 years with single-lab adoption**
- Multi-lab adoption: break-even in months, not years

---

## Conclusion

CV Zebrafish represents a high-impact, low-risk investment with clear value proposition and achievable completion timeline. The project has successfully navigated the challenging architecture and foundation phase, leaving straightforward implementation tasks for MVP completion.

**Key Strengths:**

- Validated user need with guaranteed initial adoption
- Strong technical foundation with no major risks
- Clear path to completion in next iteration
- Scalability potential beyond initial client
- Multiple output opportunities (publications, presentations, recognition)

**Recommendation:**
Continue funding for one additional development iteration to complete MVP, conduct user testing, and prepare for broader release. This investment will unlock all previously invested effort and position the tool for organic growth in the research community.

---

## Appendix: Technical Specifications

### System Requirements

- **OS:** Windows 10+, macOS 10.14+, Linux (Ubuntu 20.04+)
- **Python:** 3.10
- **RAM:** 4 GB minimum, 8 GB recommended
- **Storage:** 500 MB for software, variable for datasets

### Supported Input Formats

- **CSV:** DeepLabCut multi-animal or single-animal format
- **JSON:** Custom configuration schema (auto-generated or manual)

### Output Formats

- **Data:** CSV (calculated metrics)
- **Graphs:** PNG, SVG (from Plotly)
- **Reports:** TXT, Markdown (future)

### Performance Benchmarks

- **Validation:** <1 second for typical CSV (10,000 frames)
- **Calculation:** ~2-3 seconds for full metric suite
- **Rendering:** ~1 second per Plotly graph

### Key Dependencies

```yaml
Python: 3.10
GUI: PyQt5, PyQtWebEngine
Data: numpy, pandas, scipy
Visualization: plotly, matplotlib, kaleido
DeepLabCut: deeplabcut
Testing: unittest, pytest
Environment: conda
```

### Repository Structure

```
cv_zebrafish/
â”œâ”€â”€ app.py                       # Main application entry point
â”œâ”€â”€ environment.yml              # Conda environment specification
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ widgets/                 # PyQt5 UI components
â”‚       â”œâ”€â”€ MainWindow.py        # Primary application window
â”‚       â”œâ”€â”€ CSVInputScene.py     # CSV file selection
â”‚       â”œâ”€â”€ JSONInputScene.py    # JSON config selection
â”‚       â”œâ”€â”€ ConfigGeneratorScene.py  # Config builder UI
â”‚       â”œâ”€â”€ CalculationScene.py  # Calculation orchestration
â”‚       â””â”€â”€ GraphViewerScene.py  # Plotly graph display
â”œâ”€â”€ data_schema_validation/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ csv_verifier.py      # CSV format validation
â”‚   â”‚   â”œâ”€â”€ json_verifier.py     # JSON schema validation
â”‚   â”‚   â””â”€â”€ generate_json.py     # Auto-config generation
â”‚   â””â”€â”€ test/
â”‚       â””â”€â”€ unit_test.py         # Validation tests
â”œâ”€â”€ calculations/
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ Driver.py            # Calculation pipeline orchestrator
â”‚   â”‚   â”œâ”€â”€ Metrics.py           # Kinematic calculation functions
â”‚   â”‚   â”œâ”€â”€ Parser.py            # DLC CSV parser
â”‚   â”‚   â””â”€â”€ configSetup.py       # Configuration loading
â”‚   â””â”€â”€ tests/                   # Calculation unit tests
â”œâ”€â”€ backend/                     # Database (future)
â””â”€â”€ Meeting Minutes/             # Project documentation
```

### Contact Information

- **Tech Lead:** Madhuritha Alle (@madhuritha22)
- **Client:** Dr. Mohini Sengupta (Sengupta Bio Lab, SLU)
- **Faculty Advisor:** Dr. Daniel Shown (Computer Science, SLU)
- **Repository:** https://github.com/oss-slu/cv_zebrafish

---

_Document prepared for boardroom presentation stakeholder review_
_Last updated: October 31, 2024_
